#getting ready
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
library(phyloseq)

path <- "~/Desktop/sueyoshi/Analize＆Datas/data_YAKUSHIMA/230614-0544_Chikae_Tatsumi-7672/fastq_Lane1/for_AMF"  ## CHANGE ME to the directory containing the fastq files.
list.files(path)

fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))

#generate matched lists of the forward and reverse read files, as well as parsing out the sample name.
FWD <- "GGAAACCAAAGTGTTTGGGTTC"  ## CHANGE ME to your forward primer sequence
REV <- "GGCACCAGACTTGCCCTCCAA"  ## CHANGE ME...

#Identify the promers
allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients

#we are going to "pre-filter" the sequences just to remove those with Ns(ambiguous bases),but perform no other filtering.
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

#We are now ready to count the number of times the primers appear in the forward and reverse read, while considering all possible primer orientations.
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))


#Remove Primers
cutadapt <- "/Users/forestry/opt/miniconda3/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R

#We now create output filenames for the cutadapt-ed files, and define the parameters we are going to give the cutadapt command. 
path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i],"--minimum-length", 1)) # input files
}
#I added ","--minimum-length", 1" ,according to here(https://github.com/benjjneb/dada2/issues/1385).

#As a sanity check, we will count the presence of primers in the first cutadapt-ed sample.
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2_001.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)


#Inspect read quality profiles
#We start by visualizing the quality profiles of the forward and reverse reads:
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])

#Filter and trim : Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files.
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

#For this dataset, we will use standard filtering paraments: maxN=0 (DADA2 requires sequences contain no Ns), truncQ = 2, rm.phix = TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read,
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), 
                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)

#Learn the Error Rate
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)

#Visualize the estimated error rates as a sanity check.
plotErrors(errF, nominalQ = TRUE)

#Dereplicate identical reads
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

#Sample Inference
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

#Merge paired reads
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

#Construct Sequence Table
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

#Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)

table(nchar(getSequences(seqtab.nochim)))

#Track reads through the pipeline
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
                                                                       getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", 
                     "nonchim")
rownames(track) <- sample.names
head(track)
write.table(track,file="track.txt")

#Assign taxonomy
unite.ref <- "/Users/forestry/Desktop/sueyoshi/Analize＆Datas/for_AMF/database/not_VTtype/claered_database.fasta"  # CHANGE ME to location on your machine
taxa <- assignTaxonomy(seqtab.nochim, unite.ref, minBoot = 50, multithread = TRUE, tryRC = TRUE)

taxa.print <- taxa  # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

write.table(taxa,file="taxonomy.txt")
write.table(seqtab.nochim,file="seqtabnochim.txt")

samples.out<-rownames(seqtab.nochim)
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))

#store the DNA sequences of our ASVs in the refseq slot of the phyloseq object
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps

#別でNCBI用のデータを作っている
asv_dna_df <- data.frame(ASV = taxa_names(ps), DNA_Sequence = as.character(dna))
write.csv(asv_dna_df, file = "ASV_DNA_Mapping.csv", row.names = FALSE)

# Remove 
ps_removed = subset_taxa(ps, 
                         Family  != "Mitochondria" &
                           Class   != "Chloroplast" &
                           Kingdom  != "NA")

#To output OTU table
otu_table.t<-t(ps_removed@otu_table)
ps.t<-cbind(otu_table.t,ps_removed@tax_table)
write.table(ps.t,  file="ASV_table.txt")

# Rarefication
ps.rarefied = rarefy_even_depth(ps_removed, rngseed=1, sample.size=0.9*min(sample_sums(ps_removed)), replace=F)
otu_table.t<-t(ps.rarefied@otu_table)
ps.t<-cbind(otu_table.t,ps.rarefied@tax_table)
write.table(ps.t,  file="rarefied_ASV_table.txt")

#####

#To output OTU table without remove
otu_table.t<-t(ps@otu_table)
ps.t<-cbind(otu_table.t,ps@tax_table)
write.table(ps.t,  file="ASV_table_2.txt")

# Rarefication
ps.rarefied = rarefy_even_depth(ps, rngseed=1, sample.size=0.9*min(sample_sums(ps)), replace=F)
otu_table.t<-t(ps.rarefied@otu_table)
ps.t<-cbind(otu_table.t,ps.rarefied@tax_table)
write.table(ps.t,  file="rarefied_ASV_table_2.txt")

# データベースのフォーマットを書き換えるとき。
#fastaファイルの読み込み
fasta <- readLines("/Users/forestry/Desktop/sueyoshi/Analize＆Datas/for_AMF/database/not_VTtype/maarjam_database_SSU.fasta")
# ヘッダーを書き換える
modified_fasta <- character(length(fasta))
for (i in seq_along(fasta)) {
  if (startsWith(fasta[i], ">")) {
    header_parts <- strsplit(fasta[i], " ")[[1]]
    header_parts[length(header_parts)] <- paste0(header_parts[length(header_parts)], ";")
    taxonomy <- paste(header_parts[1:length(header_parts)], collapse = " ;")
    modified_header <- taxonomy
    modified_fasta[i] <- modified_header
  } else {
    modified_fasta[i] <- fasta[i]
  }
}

# 書き換えたfastaファイルを保存
writeLines(modified_fasta, "output2.fasta")

#####

# Output both ASVs and sequences
#Assign taxonomy
#unite.ref <- "/Users/forestry/Desktop/sueyoshi/Analize＆Datas/for_AMF/database/not_VTtype/output.fasta"  # CHANGE ME to location on your machine
unite.ref <- "/Users/forestry/Desktop/sueyoshi/Analize＆Datas/for_AMF/database/not_VTtype/output.fasta"
taxa <- assignTaxonomy(seqtab.nochim, unite.ref, minBoot = 50, multithread = TRUE, tryRC = TRUE)

taxa.print <- taxa  # Removing sequence rownames for display only
#rownames(taxa.print) <- NULL
head(taxa.print)

samples.out<-rownames(seqtab.nochim)
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               tax_table(taxa))
#store the DNA sequences of our ASVs in the refseq slot of the phyloseq object

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps

#output ASV and DNA sequences
asv_dna_df <- data.frame(ASV = taxa_names(ps), DNA_Sequence = as.character(dna))
write.csv(asv_dna_df, file = "ASV_DNA_Mapping.csv", row.names = FALSE)

# Select the top 20 ASVs
top_20_asv <- asv_dna_df[1:20, ]

# Create a DNAStringSet object for the DNA sequences
dna_sequences <- DNAStringSet(top_20_asv$DNA_Sequence)

# Create a FASTA format string for the DNA sequences
fasta_string <- paste(sprintf(">%s\n%s", top_20_asv$ASV, dna_sequences), collapse = "\n")

# Write the FASTA string to a file
writeLines(fasta_string, "ASV_DNA_Mapping.fasta")

